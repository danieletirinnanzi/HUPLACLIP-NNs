exp_name: grid_exp
graph_size_values: 
# NOTE: to perform single experiments, simply comment the graph size values that are not needed
  - 100
#  - 150
#  - 200
#  - 300
  - 400
#  - 480
#  - 600
#  - 800
  - 1200  
  
# type of correction to use (in graphs_generation.py):
# - p_increase (old correction): increasing the p value of the graph without the clique so that the average degree is matched between the two graphs
# - p_reduce (new correction): reducing the p value of the graph where the clique will be added
p_correction_type: p_reduce

training_parameters:
  # training steps:
  num_training_steps: 1000
  # training and validation set sizes:
  num_train: 24
  num_val: 24
  # other hyperparameters:
  learning_rate: 0.0001  # TO ADJUST
  # ADDITIONAL PARAMETERS GO HERE AND WILL BE READ IN train_test.py (line 106)
  # training structure:
  max_clique_size_proportion: 0.5 # during training, clique will be at most 50% of the graph size
  # min_clique_size_proportion: 0.3 NOT USED, min clique size is statistical limit
  clique_training_levels: 10
  save_step: 5
  # optimizer and loss function:
  optimizer: AdamW
  loss_function: BCELoss
  # early stopping:
  patience: 100  # since this makes the model move to the following training instance, it is more stringent
  min_delta: 0.01
  val_exit_loss: 0.1

testing_parameters:
  max_clique_size_proportion_test: 0.7 # during testing, clique will be at most 70% of the graph size
  num_test: 24  # number of graphs in each test iteration
  clique_testing_levels: 100  # number of different clique sizes to test (will be the number of datapoints in the test set)
  test_iterations: 16 # number of times for which a single clique size is tested


models:
# NOTE: to perform single experiments, simply comment the models that are not needed.
# Each instance of "models" will be trained and tested
  
  - model_name: MLP
    architecture:
      l1: 1000
      l2: 500
      l3: 100
      l4: 50
      dropout_prob: 0.2
      learning_rate: 0.01      

  # CNN models (http://layer-calc.com/ -> NB: in Pytorch, by default, in "Maxpool2D" stride == kernel_size, unless otherwise specified)
  # SMALL FILTERS, MANY LAYERS 
  # - model_name: CNN_small_1   # Kernel size 3, pooling size 3
  #   architecture:
  #     num_conv_layers: 5    # Increased number of layers
  #     c0: 1
  #     c1: 16
  #     c2: 32
  #     c3: 64
  #     c4: 128
  #     c5: 256
  #     l1: 1000
  #     stride: 1
  #     padding: 1
  #     dropout_prob: 0.3
  #     kernel_size_conv: 3
  #     kernel_size_pool: 3   # Pooling kernel size 3 for downsampling
  #     # OUT: (256, 9, 9)  # Final feature map size will depend on input size of 2400x2400

  - model_name: CNN_small_2   # Kernel size 3, pooling size 2
    architecture:
      num_conv_layers: 7    # Added more layers to further downsample
      c0: 1
      c1: 16
      c2: 32
      c3: 64
      c4: 128
      c5: 256
      c6: 512
      c7: 1024
      l1: 1000
      stride: 1
      padding: 1
      dropout_prob: 0.3
      kernel_size_conv: 3
      kernel_size_pool: 2   # Pooling kernel size 2 for faster downsampling
      # OUT: (1024, 18, 18)

  # # LARGE FILTERS, FEW LAYERS
  # - model_name: CNN_large_1   # Kernel size 20, pooling size 3
  #   architecture:
  #     num_conv_layers: 4    # Fewer layers but larger filters
  #     c0: 1
  #     c1: 16
  #     c2: 32
  #     c3: 64
  #     c4: 128
  #     l1: 1000
  #     stride: 1
  #     padding: 1
  #     dropout_prob: 0.3
  #     kernel_size_conv: 20   # Larger kernel for bigger receptive fields
  #     kernel_size_pool: 3   # Pooling kernel size 3 for downsampling
  #     # OUT: (128, 21, 21)

  - model_name: CNN_large_2   # Kernel size 20, pooling size 2
    architecture:
      num_conv_layers: 6
      c0: 1
      c1: 32
      c2: 64
      c3: 128
      c4: 256
      c5: 512
      c6: 1024
      l1: 1000
      stride: 1
      padding: 1
      dropout_prob: 0.3
      kernel_size_conv: 20
      kernel_size_pool: 2   # Pooling kernel size 2 for faster downsampling
      # OUT: (1024, 20, 20)

  # # MEDIUM FILTERS, MEDIUM LAYERS
  # - model_name: CNN_medium_1   # Kernel size 9, pooling size 3
  #   architecture:
  #     num_conv_layers: 4   # Added more layers for better control of downsampling
  #     c0: 1
  #     c1: 16
  #     c2: 32
  #     c3: 64
  #     c4: 128
  #     l1: 1000
  #     stride: 1
  #     padding: 1
  #     dropout_prob: 0.3
  #     kernel_size_conv: 9
  #     kernel_size_pool: 3   # Pooling kernel size 3 for moderate downsampling
  #     # OUT: (128, 26, 26)

  - model_name: CNN_medium_2   # Kernel size 9, pooling size 2
    architecture:
      num_conv_layers: 6   # Similar to medium_1 but with pooling size 2
      c0: 1
      c1: 16
      c2: 32
      c3: 64
      c4: 128
      c5: 256
      c6: 512
      l1: 1000
      stride: 1
      padding: 1
      dropout_prob: 0.3
      kernel_size_conv: 9
      kernel_size_pool: 2   # Pooling kernel size 2 for faster downsampling
      # OUT: (512, 31, 31)
    
  - model_name: ViTscratch

  - model_name: ViTpretrained